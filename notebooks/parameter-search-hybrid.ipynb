{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dotenv import load_dotenv\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from json import dump as json_dump, load as json_load\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import joblib\n",
    "import os\n",
    "import polars as pl\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the directory of the current file\n",
    "__dir__ = Path(os.path.abspath(\"\"))\n",
    "\"\"\"\n",
    "The directory of the current file\n",
    "\"\"\"\n",
    "\n",
    "# Load environment variables\n",
    "DATASET_NAME = os.environ[\"DATASET_NAME\"]\n",
    "\"\"\"\n",
    "Dataset name\n",
    "\"\"\"\n",
    "\n",
    "EMBEDDING_MODEL_NAME = os.environ[\"EMBEDDING_MODEL_NAME\"]\n",
    "\"\"\"\n",
    "Embedding model name\n",
    "\"\"\"\n",
    "\n",
    "CLASSIFIER_MODEL_NAME = os.environ[\"CLASSIFIER_MODEL_NAME\"]\n",
    "\"\"\"\n",
    "Classifier model name\n",
    "\"\"\"\n",
    "\n",
    "# Create the output directory\n",
    "OUTPUT_DIRECTORY = __dir__ / f\"../data/notebooks/parameter-search-hybrid/{DATASET_NAME.replace(\"/\", \"-\")}/{EMBEDDING_MODEL_NAME.replace(\"/\", \"-\")}/{CLASSIFIER_MODEL_NAME.replace(\"/\", \"-\")}\"\n",
    "(OUTPUT_DIRECTORY / \"results\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cef5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    show_progress=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5829539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and classifier models\n",
    "dtype = torch.bfloat16\n",
    "tokenizer = AutoTokenizer.from_pretrained(CLASSIFIER_MODEL_NAME)\n",
    "llm_model = AutoModelForSequenceClassification.from_pretrained(CLASSIFIER_MODEL_NAME, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486547a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier pipeline\n",
    "llm_classifier = pipeline(\n",
    "  \"text-classification\",\n",
    "  model=llm_model,\n",
    "  tokenizer=tokenizer,\n",
    "  truncation=True,\n",
    "  max_length=512,\n",
    "  device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a0d7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared dataset\n",
    "test_df = pl.read_parquet(__dir__ / f\"../data/notebooks/prepare-datasets/{DATASET_NAME.replace(\"/\", \"-\")}/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1ad4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifier\n",
    "base_classifier = joblib.load(__dir__ / f\"../data/notebooks/classifier-embedding/{DATASET_NAME.replace(\"/\", \"-\")}/{EMBEDDING_MODEL_NAME.replace(\"/\", \"-\")}/random-forest.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6610d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_classify(min_benign_confidence: float, min_malicious_confidence: float):\n",
    "  \"\"\"\n",
    "  Hybrid classifier\n",
    "  \"\"\"\n",
    "\n",
    "  def hybrid_classify_worker(text: str) -> bool:\n",
    "    # Embed the text\n",
    "    embedding = embedding_model.embed_documents(texts=[text])[0]\n",
    "\n",
    "    # Classify the embedding\n",
    "    prediction = base_classifier.predict_proba([embedding])[0]\n",
    "\n",
    "    # Determine the label based on confidence\n",
    "    if prediction[0] >= min_benign_confidence:\n",
    "      return False # Benign\n",
    "    elif prediction[1] >= min_malicious_confidence:\n",
    "      return True # Malicious\n",
    "    else:\n",
    "      # Adjudicate with the LLM\n",
    "      return llm_classifier(text)[0][\"label\"] != llm_model.config.id2label[0]\n",
    "  return hybrid_classify_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359b94b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = test_df[\"label\"].to_list()\n",
    "\n",
    "def evaluate(ctx):\n",
    "  \"\"\"\n",
    "  Evaluate the hybrid classifier for the given parameters\n",
    "  \"\"\"\n",
    "\n",
    "  # Unpack the context\n",
    "  index = ctx[\"index\"]\n",
    "  params = ctx[\"params\"]\n",
    "  min_benign_confidence = params[\"min_benign_confidence\"]\n",
    "  min_malicious_confidence = params[\"min_malicious_confidence\"]\n",
    "\n",
    "  try:\n",
    "    with open(OUTPUT_DIRECTORY / f\"results/{index}.json\", \"x\") as result_file:\n",
    "      # Classify the test dataset\n",
    "      start = time()\n",
    "      y_predictions = test_df[\"text\"].map_elements(\n",
    "        hybrid_classify(min_benign_confidence, min_malicious_confidence)\n",
    "      )\n",
    "      end = time()\n",
    "\n",
    "      elapsed_test_classify = end - start\n",
    "\n",
    "      # Get the metrics\n",
    "      auc = roc_auc_score(y_actual, y_predictions)\n",
    "      report = classification_report(y_actual, y_predictions, target_names=[\"benign\", \"malicious\"], output_dict=True)\n",
    "      \n",
    "      # Save the results\n",
    "      result = {\n",
    "        \"index\": index,\n",
    "        \"params\": params,\n",
    "        \"time\": elapsed_test_classify,\n",
    "        \"auc\": auc,\n",
    "        \"report\": report,\n",
    "      }\n",
    "\n",
    "      json_dump(result, result_file, indent=4)\n",
    "  except FileExistsError as err:\n",
    "    print(f\"Result for index {index} already exists ({err}). Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7946e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the contexts\n",
    "param_grid = {\n",
    "    \"min_benign_confidence\": [i / 10 for i in range(0, 11)],\n",
    "    \"min_malicious_confidence\": [i / 10 for i in range(0, 11)],\n",
    "}\n",
    "contexts = [\n",
    "  {\n",
    "    \"index\": i,\n",
    "    \"params\": dict(zip(param_grid.keys(), values))\n",
    "  } for i, values in enumerate(product(*param_grid.values()))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff6741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "for ctx in tqdm(contexts):\n",
    "  evaluate(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeadef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "results = []\n",
    "for filename in glob(str((OUTPUT_DIRECTORY / \"results/*.json\").resolve())):\n",
    "    with open(filename, \"r\") as result_file:\n",
    "        raw_result = json_load(result_file)\n",
    "        result = {\n",
    "            \"index\": raw_result[\"index\"],\n",
    "            \"auc\": raw_result[\"auc\"],\n",
    "            \"time\": raw_result[\"time\"],\n",
    "            **({\n",
    "                f\"params_{k}\": v\n",
    "                for k, v in raw_result[\"params\"].items()\n",
    "            } | {\n",
    "                f\"report_{k}\": v\n",
    "                for k, v in raw_result[\"report\"].items() if k in [\"accuracy\"]\n",
    "            } | {\n",
    "                f\"report_{k1}_{k2}\": v2\n",
    "                for k1, v1 in raw_result[\"report\"].items() if k1 in [\"benign\", \"malicious\", \"macro avg\", \"weighted avg\"]\n",
    "                for k2, v2 in v1.items() if k2 in [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "            })\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "results_df = pl.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171955a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 11\n",
    "COLS = 11\n",
    "VISIBLE_PARAMETERS = {\n",
    "    \"min_benign_confidence\": \"Minimum Benign Confidence\",\n",
    "    \"min_malicious_confidence\": \"Minimum Malicious Confidence\",\n",
    "}\n",
    "VISIBLE_REPORT_METRICS = {\n",
    "    \"auc\": \"AUC\",\n",
    "    \"report_accuracy\": \"Accuracy\",\n",
    "    \"report_weighted avg_precision\": \"Precision\",\n",
    "    \"report_weighted avg_recall\": \"Recall\",\n",
    "    \"report_weighted avg_f1-score\": \"F1 Score\",\n",
    "}\n",
    "COLORS = [\"blue\", \"green\", \"orange\", \"red\", \"purple\", \"gray\"]\n",
    "\n",
    "fig, axs = plt.subplots(ROWS, COLS, figsize=(20, 20))\n",
    "\n",
    "# Plot the results\n",
    "max_time = max(results_df[\"time\"])\n",
    "for row, col in product(range(ROWS), range(COLS)):\n",
    "    index = row * COLS + col\n",
    "\n",
    "    # Get the result\n",
    "    result = results_df.filter(pl.col(\"index\") == index).to_dicts()[0]\n",
    "\n",
    "    # Plot the result\n",
    "    axs[row, col].set_title(f\"Result {index}\")\n",
    "    axs[row, col].bar(\n",
    "        list(VISIBLE_REPORT_METRICS.values()),\n",
    "        [result[k] for k in VISIBLE_REPORT_METRICS.keys()],\n",
    "        color=COLORS[:len(VISIBLE_REPORT_METRICS)],\n",
    "    )\n",
    "    axs[row, col].set_xticks([])\n",
    "    axs[row, col].set_xlabel(\n",
    "        f\"({\", \".join([\n",
    "            str(result[f\"params_{k}\"])\n",
    "            for k in VISIBLE_PARAMETERS.keys()\n",
    "        ])})\"\n",
    "    )\n",
    "    axs[row, col].set_ylim(0, 1)\n",
    "\n",
    "    # Secondary axis for time\n",
    "    ax2 = axs[row, col].twinx()\n",
    "    ax2.bar(\n",
    "        [\"Time (s)\"],\n",
    "        [result[\"time\"]],\n",
    "        color=COLORS[-1],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax2.set_ylim(0, max_time * 1.1)\n",
    "\n",
    "# Title and labels\n",
    "fig.suptitle(\"Hybrid Classifier Parameter Search Results\", fontsize=16, y=1)\n",
    "fig.supxlabel(f\"Hyperparameters ({\", \".join(VISIBLE_PARAMETERS.values())})\", fontsize=12)\n",
    "\n",
    "# Legend\n",
    "handles = [\n",
    "    plt.Rectangle((0, 0), 1, 1, color=color)\n",
    "    for color in COLORS\n",
    "]\n",
    "labels = [\n",
    "    *VISIBLE_REPORT_METRICS.values(),\n",
    "    \"Time (s)\"\n",
    "]\n",
    "fig.legend(handles, labels, loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIRECTORY / \"results.png\", dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
