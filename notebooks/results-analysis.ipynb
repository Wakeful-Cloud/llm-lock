{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from json import load\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7f2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory of the current file\n",
    "__dir__ = Path(os.path.abspath(\"\"))\n",
    "\"\"\"\n",
    "The directory of the current file\n",
    "\"\"\"\n",
    "\n",
    "# Create the output directory\n",
    "OUTPUT_DIRECTORY = __dir__ / \"../data/notebooks/results-analysis\"\n",
    "OUTPUT_DIRECTORY.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_latex_table(data: dict[str, dict[str, float | int]]) -> str:\n",
    "    \"\"\"\n",
    "    Format a LaTeX table from the given data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the columns\n",
    "    columns = list(next(iter(data.values())).keys())\n",
    "\n",
    "    # Format the table header\n",
    "    formatted_table = f\"\\\\begin{{tabular}}{{|l|{\n",
    "      \"r|\" * len(columns)\n",
    "    }}}\\n\\\\hline\\n{' & '.join([\"Defense\"] + columns)}\\\\\\\\\\n\\\\hline\\n\"\n",
    "\n",
    "    # Format the table rows\n",
    "    for row_name, row_data in data.items():\n",
    "        formatted_table += f\"{row_name} & \" + \" & \".join(\n",
    "            f\"{row_data[col]:.4f}\" if isinstance(row_data[col], float) or isinstance(row_data[col], int) else str(row_data[col])\n",
    "            for col in columns\n",
    "        ) + \"\\\\\\\\\\n\"\n",
    "\n",
    "    # Format the table footer\n",
    "    formatted_table += \"\\\\hline\\n\\\\end{tabular}\"\n",
    "\n",
    "    return formatted_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_static_result(path: Path):\n",
    "    \"\"\"\n",
    "    Load static results from a JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = load(file)\n",
    "\n",
    "    return {\n",
    "        \"AUC\": data[\"auc\"],\n",
    "        \"Accuracy\": data[\"report\"][\"accuracy\"],\n",
    "        \"Precision\": data[\"report\"][\"weighted avg\"][\"precision\"],\n",
    "        \"Recall\": data[\"report\"][\"weighted avg\"][\"recall\"],\n",
    "        \"F1\": data[\"report\"][\"weighted avg\"][\"f1-score\"],\n",
    "        \"Time\": data[\"total_time\"],\n",
    "    }\n",
    "\n",
    "\n",
    "DATASETS = {\n",
    "    \"ahsanayub/malicious-prompts\": \"ahsanayub/malicious-prompts\",\n",
    "    \"jayavibhav/prompt-injection-safety\": \"jayavibhav/prompt-injection-safety\",\n",
    "    \"synthetic-dataset\": \"synthetic-dataset\",\n",
    "}\n",
    "\"\"\"\n",
    "Mapping of dataset identifiers to their display names.\n",
    "\"\"\"\n",
    "\n",
    "EMBEDDING_MODELS = {\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\": \"all-MiniLM\",\n",
    "    \"thenlper/gte-large\": \"gte-large\",\n",
    "}\n",
    "\"\"\"\n",
    "Mapping of embedding model identifiers to their display names.\n",
    "\"\"\"\n",
    "\n",
    "CLASSIFIER_MODELS = {\n",
    "    \"protectai/deberta-v3-base-prompt-injection-v2\": \"deberta-v2\",\n",
    "    \"meta-llama/Prompt-Guard-86M\": \"Prompt-Guard\",\n",
    "    \"meta-llama/Llama-Prompt-Guard-2-86M\": \"Prompt-Guard-2\",\n",
    "    \"qualifire/prompt-injection-jailbreak-sentinel-v2\": \"sentinel-v2\",\n",
    "}\n",
    "\"\"\"\n",
    "Mapping of classifier model identifiers to their display names.\n",
    "\"\"\"\n",
    "\n",
    "# Load static results\n",
    "static_results = {\n",
    "    dataset_name: {\n",
    "        # Embedding classifier\n",
    "        **{\n",
    "            embedding_model_name: load_static_result(\n",
    "                __dir__\n",
    "                / f\"../data/notebooks/classifier-embedding/{dataset_id.replace(\"/\", \"-\")}/{embedding_model_id.replace(\"/\", \"-\")}/results.json\",\n",
    "            )\n",
    "            for embedding_model_id, embedding_model_name in EMBEDDING_MODELS.items()\n",
    "        },\n",
    "        # Transformer classifier\n",
    "        **{\n",
    "            classifier_model_name: load_static_result(\n",
    "                __dir__\n",
    "                / f\"../data/notebooks/classifier-transformer/{dataset_id.replace(\"/\", \"-\")}/{classifier_model_id.replace(\"/\", \"-\")}/results.json\",\n",
    "            )\n",
    "            for classifier_model_id, classifier_model_name in CLASSIFIER_MODELS.items()\n",
    "        },\n",
    "        # Hybrid classifier\n",
    "        **{\n",
    "            f\"{embedding_model_name}-{classifier_model_name}\": load_static_result(\n",
    "                __dir__\n",
    "                / f\"../data/notebooks/classifier-hybrid/{dataset_id.replace(\"/\", \"-\")}/{embedding_model_id.replace(\"/\", \"-\")}/{classifier_model_id.replace(\"/\", \"-\")}/results.json\",\n",
    "            )\n",
    "            for embedding_model_id, embedding_model_name in EMBEDDING_MODELS.items()\n",
    "            for classifier_model_id, classifier_model_name in CLASSIFIER_MODELS.items()\n",
    "        },\n",
    "    }\n",
    "    for dataset_id, dataset_name in DATASETS.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaefaa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the static results chart\n",
    "METRICS = [\"AUC\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"]\n",
    "\"\"\"\n",
    "Metrics to be displayed in the chart.\n",
    "\"\"\"\n",
    "\n",
    "# Graph the results\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for i, (dataset, results) in enumerate(static_results.items()):\n",
    "  ax = axs[i]\n",
    "  model_names = list(results.keys())\n",
    "  x = range(len(model_names))\n",
    "\n",
    "  # Bars for each metric\n",
    "  for metric in METRICS:\n",
    "    y = [results[model][metric] for model in model_names]\n",
    "    ax.bar(\n",
    "      [p + METRICS.index(metric) * 0.1 for p in x],\n",
    "      y,\n",
    "      width=0.1,\n",
    "      label=metric,\n",
    "    )\n",
    "\n",
    "  ax.set_xticks([p + 0.2 for p in x])\n",
    "  ax.set_xticklabels(model_names, rotation=45, ha=\"right\")\n",
    "  ax.set_title(f\"Performance on {dataset}\")\n",
    "  ax.set_ylabel(\"Score\")\n",
    "  ax.set_ylim(0, 1)\n",
    "\n",
    "  # Secondary axis for time\n",
    "  ax2 = ax.twinx()\n",
    "  time_values = [results[model][\"Time\"] for model in model_names]\n",
    "  time_line, = ax2.plot([p + 0.2 for p in x], time_values, color=\"black\", marker=\"o\", label=\"Time\")\n",
    "  ax2.set_ylabel(\"Time (s)\", color=\"black\")\n",
    "  ax2.tick_params(axis=\"y\", labelcolor=\"black\")\n",
    "\n",
    "  # Combine legends\n",
    "  handles1, labels1 = ax.get_legend_handles_labels()\n",
    "  handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "  ax.legend(handles1 + [time_line], labels1 + [\"Time\"], loc=\"lower right\", facecolor=\"white\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIRECTORY / \"static-results.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e036e8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print LaTeX table for each dataset\n",
    "for dataset, results in static_results.items():\n",
    "    print(f\"LaTeX table for {dataset}:\\n{format_latex_table(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6750e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dynamic_result(path: Path):\n",
    "    \"\"\"\n",
    "    Load dynamic results from a JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = load(file)\n",
    "\n",
    "    def calculate_fractional_metric(value: dict[str, int | float]) -> float:\n",
    "        \"\"\"\n",
    "        Calculate a fractional metric from a dictionary containing 'numerator' and 'denominator'.\n",
    "        \"\"\"\n",
    "\n",
    "        return value[\"numerator\"] / value[\"denominator\"] if value[\"denominator\"] != 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"Injection Task Utility\": calculate_fractional_metric(data[\"statistics\"][\"injection_task_utility\"]),\n",
    "        \"Utility Under Attack\": calculate_fractional_metric(data[\"statistics\"][\"utility_under_attack\"]),\n",
    "        \"Attack Success Rate\": calculate_fractional_metric(data[\"statistics\"][\"targeted_attack_success_rate\"]),\n",
    "        \"Benign Utility\": calculate_fractional_metric(data[\"statistics\"][\"benign_utility\"]),\n",
    "        \"Mean Overall Overhead\": np.mean(data[\"statistics\"][\"overall_overheads\"]),\n",
    "        \"Mean Benign Overhead\": np.mean(data[\"statistics\"][\"benign_overheads\"]),\n",
    "        \"P50 Overall Overhead\": np.percentile(data[\"statistics\"][\"overall_overheads\"], 50),\n",
    "        \"P50 Benign Overhead\": np.percentile(data[\"statistics\"][\"benign_overheads\"], 50),\n",
    "        \"P95 Overall Overhead\": np.percentile(data[\"statistics\"][\"overall_overheads\"], 95),\n",
    "        \"P95 Benign Overhead\": np.percentile(data[\"statistics\"][\"benign_overheads\"], 95),\n",
    "        \"P99 Overall Overhead\": np.percentile(data[\"statistics\"][\"overall_overheads\"], 99),\n",
    "        \"P99 Benign Overhead\": np.percentile(data[\"statistics\"][\"benign_overheads\"], 99),\n",
    "    }\n",
    "\n",
    "# Load the dynamic results data\n",
    "dynamic_results = {\n",
    "    \"none\": load_dynamic_result(\n",
    "        __dir__ / \"../data/agentdojo/none/results.json\"\n",
    "    ),\n",
    "    \"data-delimiter\": load_dynamic_result(\n",
    "        __dir__ / \"../data/agentdojo/data-delimiter/results.json\"\n",
    "    ),\n",
    "    \"embedding\": load_dynamic_result(\n",
    "        __dir__ / \"../data/agentdojo/embedding/results.json\"\n",
    "    ),\n",
    "    \"transformer-deberta-v2\": load_dynamic_result(\n",
    "        __dir__ / \"../data/agentdojo/transformer/protectai-deberta-v3-base-prompt-injection-v2/results.json\"\n",
    "    ),\n",
    "    \"hybrid-deberta-v2\": load_dynamic_result(\n",
    "        __dir__ / \"../data/agentdojo/hybrid/protectai-deberta-v3-base-prompt-injection-v2/results.json\"\n",
    "    ),\n",
    "    \"transformer-sentinel-v2\": load_dynamic_result(\n",
    "        __dir__ / \"../data/agentdojo/transformer/qualifire-prompt-injection-jailbreak-sentinel-v2/results.json\"\n",
    "    ),\n",
    "    \"hybrid-sentinel-v2\": load_dynamic_result(\n",
    "        __dir__ / \"../data/agentdojo/hybrid/qualifire-prompt-injection-jailbreak-sentinel-v2/results.json\"\n",
    "    ),\n",
    "    \"llm\": load_dynamic_result(\n",
    "        __dir__ / \"../data/agentdojo/llm/results.json\"\n",
    "    ),\n",
    "    \"multiclass\": load_dynamic_result(\n",
    "        __dir__ / \"../data/agentdojo/multiclass/results.json\"\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df9cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dynamic results charts\n",
    "PERFORMANCE_METRICS = [\n",
    "    \"Injection Task Utility\",\n",
    "    \"Utility Under Attack\",\n",
    "    \"Attack Success Rate\",\n",
    "    \"Benign Utility\",\n",
    "]\n",
    "\"\"\"\n",
    "Metrics related to task performance.\n",
    "\"\"\"\n",
    "\n",
    "LATENCY_METRICS = [\n",
    "    \"Mean Overall Overhead\",\n",
    "    \"Mean Benign Overhead\",\n",
    "    \"P50 Overall Overhead\",\n",
    "    \"P50 Benign Overhead\",\n",
    "    \"P95 Overall Overhead\",\n",
    "    \"P95 Benign Overhead\",\n",
    "    \"P99 Overall Overhead\",\n",
    "    \"P99 Benign Overhead\",\n",
    "]\n",
    "\"\"\"\n",
    "Metrics related to latency overhead.\n",
    "\"\"\"\n",
    "\n",
    "LATENCY_SKIP_DATASETS = [\n",
    "    \"none\",\n",
    "    \"data-delimiter\",\n",
    "]\n",
    "\"\"\"\n",
    "Datasets to skip for latency metrics (i.e., that don't have any overhead/latency).\n",
    "\"\"\"\n",
    "\n",
    "# Graph the dynamic results\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Graph the performance metrics\n",
    "x = range(len(dynamic_results.keys()))\n",
    "for metric in PERFORMANCE_METRICS:\n",
    "    y = [dynamic_results[defense][metric] for defense in dynamic_results.keys()]\n",
    "    axs[0].bar(\n",
    "        [p + PERFORMANCE_METRICS.index(metric) * 0.1 for p in x],\n",
    "        y,\n",
    "        width=0.1,\n",
    "        label=metric,\n",
    "    )\n",
    "\n",
    "axs[0].set_title(\"Task Performance\")\n",
    "axs[0].set_xticks([p + 0.15 for p in x])\n",
    "axs[0].set_xticklabels(dynamic_results.keys(), rotation=45, ha=\"right\")\n",
    "axs[0].set_ylim(0)\n",
    "axs[0].set_ylabel(\"Score\")\n",
    "axs[0].legend(loc=\"upper center\", facecolor=\"white\")\n",
    "\n",
    "# Graph the latency metrics\n",
    "x = range(len(dynamic_results.keys()) - len(LATENCY_SKIP_DATASETS))\n",
    "for metric in LATENCY_METRICS:\n",
    "    y = [\n",
    "        dynamic_results[defense][metric]\n",
    "        for defense in dynamic_results.keys()\n",
    "        if defense not in LATENCY_SKIP_DATASETS\n",
    "    ]\n",
    "    axs[1].bar(\n",
    "        [p + LATENCY_METRICS.index(metric) * 0.1 for p in x], y, width=0.1, label=metric\n",
    "    )\n",
    "\n",
    "axs[1].set_title(\"Latency Overhead\")\n",
    "axs[1].set_xticks([p + 0.35 for p in x])\n",
    "axs[1].set_xticklabels(\n",
    "    [\n",
    "        defense\n",
    "        for defense in dynamic_results.keys()\n",
    "        if defense not in LATENCY_SKIP_DATASETS\n",
    "    ],\n",
    "    rotation=45,\n",
    "    ha=\"right\",\n",
    ")\n",
    "axs[1].set_yscale(\"log\")\n",
    "axs[1].set_ylim(0, 5)\n",
    "axs[1].set_ylabel(\"Defense Overhead (Milliseconds)\")\n",
    "axs[1].legend(loc=\"upper center\", facecolor=\"white\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIRECTORY / \"dynamic-results.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8271d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print LaTeX table\n",
    "print(f\"Dynamic results:\\n{format_latex_table(dynamic_results)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
