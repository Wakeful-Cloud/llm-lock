{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b8a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dotenv import load_dotenv\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from json import dump as json_dump, load as json_load\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f4fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get the directory of the current file\n",
    "__dir__ = Path(os.path.abspath(\"\"))\n",
    "\"\"\"\n",
    "The directory of the current file\n",
    "\"\"\"\n",
    "\n",
    "# Load environment variables\n",
    "DATASET_NAME = os.environ[\"DATASET_NAME\"]\n",
    "\"\"\"\n",
    "Dataset name\n",
    "\"\"\"\n",
    "\n",
    "EMBEDDING_MODEL_NAME = os.environ[\"EMBEDDING_MODEL_NAME\"]\n",
    "\"\"\"\n",
    "Embedding model name\n",
    "\"\"\"\n",
    "\n",
    "# Create the output directory\n",
    "OUTPUT_DIRECTORY = __dir__ / f\"../data/notebooks/parameter-search-embedding/{DATASET_NAME.replace(\"/\", \"-\")}/{EMBEDDING_MODEL_NAME.replace(\"/\", \"-\")}\"\n",
    "(OUTPUT_DIRECTORY / \"results\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedded datasets\n",
    "train_df = pl.read_parquet(__dir__ / f\"../data/notebooks/classifier-embedding/{DATASET_NAME.replace(\"/\", \"-\")}/{EMBEDDING_MODEL_NAME.replace(\"/\", \"-\")}/train-embedded.parquet\")\n",
    "test_df = pl.read_parquet(__dir__ / f\"../data/notebooks/classifier-embedding/{DATASET_NAME.replace(\"/\", \"-\")}/{EMBEDDING_MODEL_NAME.replace(\"/\", \"-\")}/test-embedded.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(ctx):\n",
    "    \"\"\"\n",
    "    Train and evaluate the classifier for the given parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the context\n",
    "    index = ctx[\"index\"]\n",
    "    params = ctx[\"params\"]\n",
    "\n",
    "    try:\n",
    "        with open(OUTPUT_DIRECTORY / f\"results/{index}.json\", \"x\") as result_file:\n",
    "            # Train the classifier\n",
    "            # See https://github.com/AhsanAyub/malicious-prompt-detection/blob/main/binary_classification.py\n",
    "            classifier = RandomForestClassifier(criterion=\"gini\", random_state=0, verbose=2, **params)\n",
    "            classifier.fit(\n",
    "                train_df[\"embeddings\"].to_list(), \n",
    "                train_df[\"label\"].to_list()\n",
    "            )\n",
    "\n",
    "            # Test the classifier\n",
    "            y_predictions = classifier.predict(test_df[\"embeddings\"].to_list())\n",
    "\n",
    "            # Get the metrics\n",
    "            y_actual = test_df[\"label\"].to_list()\n",
    "\n",
    "            auc = roc_auc_score(y_actual, y_predictions)\n",
    "            report = classification_report(y_actual, y_predictions, target_names=[\"benign\", \"malicious\"], output_dict=True)\n",
    "\n",
    "            # Save the results\n",
    "            result = {\n",
    "                \"index\": index,\n",
    "                \"params\": params,\n",
    "                \"auc\": auc,\n",
    "                \"report\": report,\n",
    "            }\n",
    "\n",
    "            json_dump(result, result_file, indent=4)\n",
    "    except FileExistsError as err:\n",
    "        print(f\"Result for index {index} already exists ({err}). Skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the contexts\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100, 200],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "contexts = [\n",
    "  {\n",
    "    \"index\": i,\n",
    "    \"params\": dict(zip(param_grid.keys(), values))\n",
    "  } for i, values in enumerate(product(*param_grid.values()))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd4b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "for ctx in tqdm(contexts):\n",
    "  train_and_evaluate(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2139e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "results = []\n",
    "for filename in glob(str((OUTPUT_DIRECTORY / \"results/*.json\").resolve())):\n",
    "    with open(filename, \"r\") as result_file:\n",
    "        raw_result = json_load(result_file)\n",
    "        result = {\n",
    "            \"index\": raw_result[\"index\"],\n",
    "            \"auc\": raw_result[\"auc\"],\n",
    "            **({\n",
    "                f\"params_{k}\": v\n",
    "                for k, v in raw_result[\"params\"].items()\n",
    "            } | {\n",
    "                f\"report_{k}\": v\n",
    "                for k, v in raw_result[\"report\"].items() if k in [\"accuracy\"]\n",
    "            } | {\n",
    "                f\"report_{k1}_{k2}\": v2\n",
    "                for k1, v1 in raw_result[\"report\"].items() if k1 in [\"benign\", \"malicious\", \"macro avg\", \"weighted avg\"]\n",
    "                for k2, v2 in v1.items() if k2 in [\"precision\", \"recall\", \"f1-score\", \"support\"]\n",
    "            })\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "results_df = pl.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41792dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROWS = 9\n",
    "COLS = 9\n",
    "VISIBLE_PARAMETERS = {\n",
    "    \"n_estimators\": \"Number of Estimators\",\n",
    "    \"max_depth\": \"Max Depth\",\n",
    "    \"min_samples_split\": \"Min Samples Split\",\n",
    "    \"min_samples_leaf\": \"Min Samples Leaf\",\n",
    "}\n",
    "VISIBLE_REPORT_METRICS = {\n",
    "    \"auc\": \"AUC\",\n",
    "    \"report_accuracy\": \"Accuracy\",\n",
    "    \"report_weighted avg_precision\": \"Precision\",\n",
    "    \"report_weighted avg_recall\": \"Recall\",\n",
    "    \"report_weighted avg_f1-score\": \"F1 Score\",\n",
    "}\n",
    "COLORS = [\"blue\", \"green\", \"orange\", \"red\", \"purple\"]\n",
    "\n",
    "fig, axs = plt.subplots(ROWS, COLS, figsize=(20, 20))\n",
    "\n",
    "# Plot the results\n",
    "best_auc_index = results_df.sort(\"auc\", descending=True)[0, \"index\"]\n",
    "for row, col in product(range(ROWS), range(COLS)):\n",
    "    index = row * COLS + col\n",
    "\n",
    "    # Get the result\n",
    "    result = results_df.filter(pl.col(\"index\") == index).to_dicts()[0]\n",
    "\n",
    "    # Plot the result (Make the title bold if result is best)\n",
    "    axs[row, col].set_title(f\"Result {index}\", fontweight=\"bold\" if index == best_auc_index else \"normal\")\n",
    "    axs[row, col].bar(\n",
    "        list(VISIBLE_REPORT_METRICS.values()),\n",
    "        [result[k] for k in VISIBLE_REPORT_METRICS.keys()],\n",
    "        color=COLORS,\n",
    "    )\n",
    "    axs[row, col].set_xticks([])\n",
    "    axs[row, col].set_xlabel(\n",
    "        f\"({\", \".join([\n",
    "            str(result[f\"params_{k}\"] or \"-\")\n",
    "            for k in VISIBLE_PARAMETERS.keys()\n",
    "        ])})\"\n",
    "    )\n",
    "    axs[row, col].set_ylim(0, 1)\n",
    "\n",
    "# Title and labels\n",
    "fig.suptitle(\"Random Forest Classifier Parameter Search Results\", fontsize=16, y=1)\n",
    "fig.supxlabel(f\"Hyperparameters ({\", \".join(VISIBLE_PARAMETERS.values())})\", fontsize=12)\n",
    "\n",
    "# Legend\n",
    "handles = [\n",
    "    plt.Rectangle((0, 0), 1, 1, color=color)\n",
    "    for color in COLORS\n",
    "]\n",
    "labels = [\n",
    "    *VISIBLE_REPORT_METRICS.values(),\n",
    "]\n",
    "fig.legend(handles, labels, loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIRECTORY / \"results.png\", dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
